# nn-random-labels

## References:

[1] What Do Neural Networks Learn When Trained With Random Labels? [link](https://arxiv.org/pdf/2006.10455.pdf)

[2] What is being transferred in transfer learning? [link](https://papers.nips.cc/paper/2020/file/0607f4c705595b911a4f3e7a127b44e0-Paper.pdf)

[3] Train-by-Reconnect: Decoupling Locations of Weights from their Values [link](https://arxiv.org/pdf/2003.02570.pdf)

[4] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks [link](https://arxiv.org/pdf/1803.03635.pdf)

[5] Predicting accuracy of DNN's from weights [link](https://github.com/CalculatedContent/WeightWatcher)
## Thoughts:

- Dependence on network size/# of training epochs?

- What kind of additional structure can be found on weights of these (or general) neural networks?

- What happens in domains outside of image processing?

- Is it possible to combine [3] and [4], to create a small network that you can train with reconnecting?

- Do robust networks also align with the lottery ticket hypothesis/can you train these lottery tickets robustly?
